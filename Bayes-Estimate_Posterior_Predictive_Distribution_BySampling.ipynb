{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Posterior Predictive Distribution by Sampling \n",
    "\n",
    "## Motivation\n",
    "In this notebook we will explain how to obtain an approximate predictive distribution sampling. The two main reasons that we want to obtain the posterior predictive distribution are:\n",
    "\n",
    "- to predict or to forecast new data \n",
    "- to check the model fit using what are known as \"posterior predictive checks\"\n",
    "\n",
    "The last point is a very valuable and important task that we must consider doing whenever we fit a model in the bayesian paradigm.\n",
    "\n",
    "For the posterior predictive distribution there is an exact equation, but importantly in most applied circumstances this equation requires intractable calculations to be done. This implies that we cannot obtain the exact posterior predictive distribution and thus we can overcome this difficulty by relying on a sampling approach.\n",
    "\n",
    "To start with, a posterior predictive distribution is the probability distribution over some new data denoted as $\\tilde{x}$ given that we have observed our sample $X$: $P(\\tilde{x}|X)$. A posterior predictive distribution accounts for uncertainty about ${\\displaystyle \\theta }$. The posterior distribution of possible ${\\displaystyle \\theta }$ values depends on ${\\displaystyle \\mathbf {X} }$:\n",
    "\n",
    "\\begin{equation}\n",
    "{\\displaystyle p(\\theta |\\mathbf {X} )}\n",
    "\\end{equation}\n",
    "\n",
    "The posterior predictive distribution of ${\\displaystyle {\\tilde {x}}}$ given ${\\displaystyle \\mathbf {X} }$  is calculated by marginalizing the distribution of ${\\displaystyle {\\tilde {x}}}$ given ${\\displaystyle \\theta }$  over the posterior distribution of ${\\displaystyle \\theta }$  given ${\\displaystyle \\mathbf {X} }$:\n",
    "\n",
    "\\begin{equation}\n",
    "{\\displaystyle p({\\tilde {x}}|\\mathbf {X} )=\\int _{\\Theta }p({\\tilde {x}}|\\theta ,\\mathbf {X} )\\,p(\\theta |\\mathbf {X} )\\operatorname {d} \\!\\theta }\n",
    "\\end{equation}\n",
    "\n",
    "Because it accounts for uncertainty about ${\\displaystyle \\theta }$, the posterior predictive distribution will in general be wider than a predictive distribution which plugs in a single best estimate for ${\\displaystyle \\theta }$ .\n",
    "\n",
    "In practice, we approximate the predictive distribution $P(\\tilde{x}|X)$ using sampling in two steps:\n",
    "\n",
    "- __Step 1__: In the first step, we obtain a value of the model parameter $\\theta_i$ by indepentently sampling from the posterior distribution: $\\theta_i \\sim P(\\theta|X)$\n",
    "- __Step 2__: Next, what we do is we use that value of $\\theta_i$ and we plug that into the sampling distribution. Then, we sample a value of $\\tilde{x}$, which is conditional on that value of $\\theta_i$: $\\tilde{x}_i \\sim P(\\tilde{X}|\\theta_i)$.\n",
    "\n",
    "The above process is then repeated for many times and if we draw a historgram of the $\\tilde{x}_i$ values, we expect it to be an approximation to our posterior predicted distribution:\n",
    "\n",
    "\n",
    "$\\mathrm{for}\\, i=1:\\mathrm{many}\\{$\n",
    "\n",
    "$\\qquad P(\\tilde{X}|X) \\quad \\leftarrow \\quad  \\tilde{x}_i \\sim P(\\tilde{X}|\\theta_i) \\quad \\leftarrow \\quad \\theta_i \\sim P(\\theta|X)$\n",
    "\n",
    "$\\}$\n",
    "\n",
    "\n",
    "Note that, each of those two steps represent the two sources of uncertainty that we deal with in predicting new data. Firstly, there is uncertainty on the value of the parameter $\\theta$ and so that is dictated by the posterior distribution. Secondly, there is the uncertainty in the data generating process itself; if we even didn't know what $\\theta$ was, there is still a certain factor of randomness in that process. The latter source of uncertainty is represented by the second (middle) step of this sampling process.\n",
    "\n",
    "## Example\n",
    "\n",
    "To provide a thourough explanation of how this technique works, we are going to go through an example now. Let's start up a problem by imagining we have got a sample of $n$ school students, and in that sample we count the number that wear myopia glasses, $m$. We assume that always the number of students with near-sightedness are less or equal from the total population, i.e. $m \\leq n$. \n",
    "\n",
    "To come up with a suitable likelihood we need to consider the following facts:\n",
    "\n",
    "1. the data sample size is fixed\n",
    "2. the observed outcome $X$ is a discrete random variable. In the case we are measuring the number of students whose vision condition exceed some given threshold, we say to have discrete random variable.\n",
    "\n",
    "and the following assumptions:\n",
    "\n",
    "1. the vision conditions of a student is independent of another student. This means that, if we know the vision condition of one student, it does not help us predict the vision condition of another student, apart from the fact that both come from the same population. This assumption will be violated if we take students from the same family, or they are twins, because you expect then those students to be more similar than children in the overall opulation. Supposing that, what we are interested here is inferring the myopia rate of school students in the population. \n",
    "2. the students come from the same poplulation. In other words, their data are identically and independently distributed (iid). This assumptio will be violated if some of the student coming from one population and another portion is coming from a different population.\n",
    "\n",
    "By combining the above facts and assumptions together, we obtain a _binomial_ likelihood. So, in this example we are going to use a binomial probability mass function (pmf), assuming that $X$ is binomialy distributed with a sample size of $n$ and a parameter $p$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{B}(n,\\,p)\n",
    "\\end{equation}\n",
    "\n",
    "where $p=\\theta$ represents the proportion of the population of students which wear myopia glasses. In general, if the random variable $X$ follows the binomial distribution with parameters $n \\in \\mathbb{N}$ and $p \\in [0,\\,1]$, we write $X \\sim B(n,\\, p)$. Then we can use a beta function \n",
    "\n",
    "\\begin{equation}\n",
    "{\\displaystyle \\beta (x,y)=\\int _{0}^{1}t^{x-1}(1-t)^{y-1}\\,dt}\n",
    "\\end{equation}\n",
    "\n",
    "for complex number inputs $x$, $y$ such that Re $x > 0$, Re $y > 0$, as prior on $\\theta$. The benefit of using a $\\beta$ function prior is that is conjugated to a binomial likelihood, which means we can exactly calculate the posterior. We get that $\\theta$ conditioned on $X$ \n",
    "\n",
    "\\begin{equation}\n",
    "\\theta | X \\sim \\beta(a,\\,b)\n",
    "\\end{equation}\n",
    "\n",
    "is a $\\beta$ distribution with the first argument being $a=1+m$ and the second argument being $b=1+n-m$. For example, if $n=10$ with $m=2$ then we obtain a beta $\\beta(3,\\,9)$ as our posterior. \n",
    "\n",
    "So, we are going to be sampling from a beta posterior distribution, \n",
    "\n",
    "\\begin{equation}\n",
    "\\beta(a,\\,b) \\quad \\mathrm{with} \\quad a=1+m \\quad \\mathrm{and} \\quad b=1+n-m\n",
    "\\end{equation}\n",
    "\n",
    "to obtain a value of $\\theta_i$. Hereafater, we use a binomial sampling distribution conditional on the obtained $\\theta_i$ to sample a value of $\\tilde{x}_i$. Also, we are going to assume in the example that the sample size in the predicted dataset is fixed to $n$.\n",
    "\n",
    "## Simulation\n",
    "\n",
    "Having the theory set, we can proceed to performing simulations to illustrate how we can build up the posterior predictive distribution by sampling. Let's first do the necessary imports of python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import beta, binom\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define a handy class to nicely bundle the posterior outcomes together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Posterior:\n",
    "    \"\"\"\n",
    "    Class to group the posterior results together\n",
    "    for convenient handling in drawing the distributions\n",
    "    afterwards\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 name:str=None,\n",
    "                 a:float=None,\n",
    "                 b:float=None,\n",
    "                 x:float=None,\n",
    "                 y:float=None,\n",
    "                 rv:float=None):\n",
    "        self.__name = name\n",
    "        self.__a = a    # parameter a\n",
    "        self.__b = b    # parameter b\n",
    "        self.__x = x    # x data\n",
    "        self.__y = y    # y data\n",
    "        self.__rv = rv  # random value\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.__name\n",
    "        \n",
    "    @property\n",
    "    def a(self):\n",
    "        return self.__a\n",
    "\n",
    "    @property\n",
    "    def b(self):\n",
    "        return self.__b\n",
    "\n",
    "    @property\n",
    "    def x(self):\n",
    "        return self.__x\n",
    "\n",
    "    @property\n",
    "    def y(self):\n",
    "        return self.__y\n",
    "    \n",
    "    @property\n",
    "    def rv(self):\n",
    "        return self.__rv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each iteration event we store the sampling outcomes on the parameter $\\theta$ and $\\tilde{x}$ in the next class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling:\n",
    "    \"\"\"\n",
    "    Keeps data from sampling process:\n",
    "    - sample theta from posterior distribution\n",
    "    - new X data conditioned on theta\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 θ:Posterior=None,\n",
    "                 X:Posterior=None):\n",
    "        self.__θ = θ\n",
    "        self.__X = X\n",
    "\n",
    "    @property\n",
    "    def θ(self):\n",
    "        return self.__θ\n",
    "\n",
    "    @property\n",
    "    def X(self):\n",
    "        return self.__X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All results are then collected in the following vector class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SamplingData:\n",
    "    \"\"\"\n",
    "    Collection of Sampling data\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 i:int=None,\n",
    "                 n:int=None,\n",
    "                 m:int=None,\n",
    "                 r:int=None,\n",
    "                 ε:float=None):\n",
    "        self.__i = i          #number of iterations\n",
    "        self.__n = n          #size of population\n",
    "        self.__m = m          #size of subset in population\n",
    "        self.__r = r          #size of posterior sample to be drawn\n",
    "        self.__ε = ε          #epsilon value to avoid irregularities at distribution edges\n",
    "        self.__list = []      #list of results\n",
    "        self.__X_tilde = None #sampled data conditional on theta \n",
    "\n",
    "    def __call__(self): #overloaded parentheses operator\n",
    "        return self.__list\n",
    "    \n",
    "    def add(self, x:Sampling):\n",
    "        self.__list.append(x)\n",
    "\n",
    "    @property\n",
    "    def i(self):\n",
    "        return self.__i\n",
    "        \n",
    "    @property\n",
    "    def n(self):\n",
    "        return self.__n\n",
    "\n",
    "    @property\n",
    "    def m(self):\n",
    "        return self.__m\n",
    "    \n",
    "    @property\n",
    "    def r(self):\n",
    "        return self.__r\n",
    "\n",
    "    @property\n",
    "    def ε(self):\n",
    "        return self.__ε\n",
    "    \n",
    "    @property\n",
    "    def X_tilde(self):\n",
    "        if not self.__X_tilde:\n",
    "            self.__X_tilde = [sample.X.rv for sample in self.__list]\n",
    "        return self.__X_tilde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function is useful for drawing the single posterior distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drawing helper\n",
    "def draw_sample(idx=None,\n",
    "                sample = None,\n",
    "                X_tilde = None,\n",
    "                i = None,\n",
    "                ε = 1e-6,\n",
    "                n = None,\n",
    "                m = None,\n",
    "                r = None,\n",
    "                directory = None,\n",
    "                figsize = (15,5),\n",
    "                show_graphics = False,\n",
    "                save_graphics = False,\n",
    "                ):\n",
    "\n",
    "    #for counting purposes\n",
    "    idx += 1\n",
    "    \n",
    "    #prepare canvas\n",
    "    fig, ax = plt.subplots(1, 3, figsize=figsize)\n",
    "    #fig.tight_layout()\n",
    "\n",
    "    #########################################\n",
    "    #posterior function for unknown parameter\n",
    "    #########################################\n",
    "\n",
    "    #Display the probability density function (pdf) which is\n",
    "    #the posterior from which we are sampling theta\n",
    "    ax[0].plot(sample.θ.x,\n",
    "               sample.θ.y,\n",
    "               'r-',\n",
    "               lw=2,\n",
    "               alpha=0.6,\n",
    "               label=f'${sample.θ.name}$ pdf')\n",
    "    ax[0].set_xlabel('$\\\\theta$')\n",
    "    ax[0].set_ylabel('probability density')\n",
    "    ax[0].set_title(f'n={n}, X={m} $\\Rightarrow '\n",
    "                    f'p(\\\\theta|X)={sample.θ.name}({sample.θ.a},\\\\,{sample.θ.b})$')\n",
    "    ax[0].set_xlim([0, 1])\n",
    "    ax[0].set_ylim([0, None])\n",
    "    ax[0].axvline(x=sample.θ.rv,\n",
    "                  label=f'Random value = {sample.θ.rv:.2f}',\n",
    "                  c='orange')\n",
    "    ax[0].legend(loc='upper right', frameon=False)\n",
    "    \n",
    "    ########################################\n",
    "    # data distribution conditional on theta\n",
    "    ########################################\n",
    "    ax[1].plot(sample.X.x,\n",
    "               sample.X.y,\n",
    "               'bo',\n",
    "               lw=2,\n",
    "               alpha=0.6,\n",
    "           label='binomial pmf')\n",
    "    ax[1].set_xlim([-0.5, r])\n",
    "    ax[1].set_ylim([0, None])\n",
    "    ax[1].set_xlabel('$\\\\tilde{X}$')\n",
    "    ax[1].set_ylabel('probability')\n",
    "    ax[1].set_title('$p(\\\\tilde{X}|\\\\theta_{%i})=%s(%i,\\\\,%.2f)$'%(idx,\n",
    "                                                                sample.X.name,\n",
    "                                                                sample.X.a,\n",
    "                                                                sample.X.b))\n",
    "    ax[1].vlines(sample.X.x,\n",
    "                 0,\n",
    "                 sample.X.y,\n",
    "                 colors='b',\n",
    "                 lw=5,\n",
    "                 alpha=0.5)\n",
    "    ax[1].axvline(x=sample.X.rv,\n",
    "                  label=f'Random value = {sample.X.rv}',\n",
    "                  c='orange')\n",
    "\n",
    "    ax[1].legend(loc='upper right', frameon=False)\n",
    "\n",
    "    #########################################################\n",
    "    # sample data from data distribution conditional on theta\n",
    "    #########################################################\n",
    "\n",
    "    #plot randomly sampled value\n",
    "    hy, hx, _ = ax[2].hist(X_tilde,\n",
    "                           density=False,\n",
    "                           bins=10,\n",
    "                           alpha=0.6,\n",
    "                           label='Samples')\n",
    "    #define offset\n",
    "    hy_max = hy.max()\n",
    "    offset = 1.15\n",
    "    \n",
    "    ax[2].set_xlim([-0.5, r])\n",
    "    ax[2].set_ylim([0, hy_max*offset])\n",
    "    ax[2].set_xlabel('$\\\\tilde{X}$')\n",
    "    ax[2].set_ylabel('count')\n",
    "    ax[2].set_title('$p(\\\\tilde{X}|X)$ sample=%d/%d'%(idx, i))\n",
    "\n",
    "    #indicate the bin in which the data was added\n",
    "    ax[2].arrow(X_tilde[-1],\n",
    "                hy_max*offset,\n",
    "                0,\n",
    "                -hy_max*0.05,\n",
    "                length_includes_head=False,\n",
    "                head_width=0.5,\n",
    "                head_length=hy_max*0.05)\n",
    "    \n",
    "    #iteration number\n",
    "    number_str = str(idx)\n",
    "    zero_filled_number_str = number_str.zfill(len(str(i)))\n",
    "\n",
    "    #filename\n",
    "    path = Path(directory)\n",
    "    \n",
    "    figname = path.joinpath('sampling_%s.png'%(zero_filled_number_str) )\n",
    "\n",
    "    #show graphics\n",
    "    if show_graphics: plt.show()\n",
    "\n",
    "    #store graphics\n",
    "    if save_graphics: fig.savefig(figname, dpi=fig.dpi)\n",
    "\n",
    "    #release stuff from memory\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function calls the previous one to create a bunch of posterior predictive distribtions obtained from sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw(samples:SamplingData=None,\n",
    "         directory:str=None):\n",
    "\n",
    "    #folder to keep\n",
    "    path = Path(directory)\n",
    "    path.mkdir(parents=True,\n",
    "               exist_ok=True)\n",
    "\n",
    "    for f in path.glob('*.*'):\n",
    "        f.unlink()\n",
    "    \n",
    "    #collect drawn posterior data in an increamental way\n",
    "    #so to monitor progress\n",
    "    X_tilde_increamental = []\n",
    "    \n",
    "    for isample, sample in enumerate(samples()):\n",
    "        X_tilde_increamental.append(sample.X.rv)\n",
    "        draw_sample(idx = isample,\n",
    "                    sample = sample,\n",
    "                    X_tilde = X_tilde_increamental,\n",
    "                    i = samples.i,\n",
    "                    n = samples.n,\n",
    "                    m = samples.m,\n",
    "                    ε = samples.ε,\n",
    "                    r = samples.r,\n",
    "                    directory = directory,\n",
    "                    show_graphics=False,\n",
    "                    save_graphics=True\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function helps to use each plot as frame and create an animation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate(directory:str):\n",
    "\n",
    "    image_path = Path(directory)\n",
    "    images = list(image_path.glob('*.png'))\n",
    "    frames = []\n",
    "    for file_name in sorted(images):\n",
    "        frames.append(imageio.imread(file_name))\n",
    "\n",
    "    exportfile = image_path.joinpath('animation.gif')\n",
    "    #imageio.mimwrite(exportfile,\n",
    "    #                 frames,\n",
    "    #                 fps= 100 # number of frames per second (default=10) \n",
    "    #                 )\n",
    "\n",
    "    imageio.mimsave(exportfile,\n",
    "                    frames,\n",
    "                    format='GIF',\n",
    "                    #duration=0.75,\n",
    "                    fps=1 # frames per second: the smaller, the faster\n",
    "                    )\n",
    "    #clean up local files\n",
    "    for image in images:\n",
    "        image.unlink()\n",
    "\n",
    "    print('Results in file:', exportfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define the iterative sampling process method and the  main user-callable function for running that method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_process(sampling_data:SamplingData=None, #our sampling data\n",
    "                     Niter:int=None, #number of events\n",
    "                     n:int=None,     #population size\n",
    "                     m:int=None,     #subset in population\n",
    "                     r:int=None,     #size of distr to be drawn\n",
    "                     ε=1e-5 #distribution edge offset\n",
    "                     ):\n",
    "    # fix the random seed for replicability.\n",
    "    np.random.seed(33)\n",
    "    \n",
    "    #fixed beta parameters\n",
    "    a_beta, b_beta = 1+m, 1+n-m\n",
    "\n",
    "    #fixed X data for beta\n",
    "    #use percent point function/ppf (inverse of cdf) at q for given params\n",
    "    x_beta = np.linspace(beta.ppf(0+ε, a_beta, b_beta),\n",
    "                         beta.ppf(1-ε, a_beta, b_beta),\n",
    "                         100)\n",
    "\n",
    "    #fixed y data for beta\n",
    "    y_beta = beta.pdf(x_beta, a_beta, b_beta)\n",
    "\n",
    "    #iterations\n",
    "    for i in range(Niter):\n",
    "\n",
    "        #generate random number from beta posterior distribution\n",
    "        rv_beta = beta.rvs(a_beta, b_beta, size=1)[0]\n",
    "\n",
    "        #our posterior for theta\n",
    "        posterior_θ = Posterior(name='\\\\beta',\n",
    "                                a=a_beta,\n",
    "                                b=b_beta,\n",
    "                                x=x_beta,\n",
    "                                y=y_beta,\n",
    "                                rv=rv_beta\n",
    "                                )\n",
    "    \n",
    "        #binomial distribution conditional on theta - varying params\n",
    "        a_binom, b_binom = r, rv_beta\n",
    "\n",
    "        #binomial X data - varying data\n",
    "        x_binom = np.arange(binom.ppf(0+ε, a_binom, b_binom),\n",
    "                            binom.ppf(1-ε, a_binom, b_binom))\n",
    "\n",
    "        y_binom = binom.pmf(x_binom, a_binom, b_binom)\n",
    "    \n",
    "        #generate random number from binomial distribution: x_tilde\n",
    "        rv_binom = binom.rvs(a_binom, b_binom, size=1)[0]\n",
    "    \n",
    "        #our new data conditioned on sampled theta\n",
    "        posterior_X = Posterior(name='\\\\mathrm{Binomial}',\n",
    "                                a=a_binom,\n",
    "                                b=b_binom,\n",
    "                                x=x_binom,\n",
    "                                y=y_binom,\n",
    "                                rv=rv_binom\n",
    "                                )\n",
    "\n",
    "        #keep our finding in a list\n",
    "        sampling_data.add( Sampling(θ = posterior_θ,\n",
    "                                X = posterior_X) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(Niter:int=None,     #number of events\n",
    "        directory:str=None, #output dir\n",
    "        n:int=None,         #population size\n",
    "        m:int=None,         # subset in population\n",
    "        r:int=None,         # size of population to be drawn\n",
    "        ε=1e-5              #distribution edge offset\n",
    "       ):\n",
    "    \n",
    "    # rename directory to avoid name clashing\n",
    "    directory += f'_i{Niter}_n{n}_m{m}_r{r}'\n",
    "    \n",
    "    #list of sampling results\n",
    "    sampling_data = SamplingData(i=Niter, n=n, m=m, r=r, ε=ε)\n",
    "    \n",
    "    #sampling\n",
    "    sampling_process(sampling_data=sampling_data,\n",
    "                     Niter=Niter,\n",
    "                     n=n,\n",
    "                     m=m,\n",
    "                     r=r)\n",
    "    \n",
    "    # draw sampling data\n",
    "    draw(samples=sampling_data,\n",
    "         directory=directory)\n",
    "\n",
    "    # animate\n",
    "    animate(directory=directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all tools in our hands to run our first example on drawing a posterior predictive distribution by sampling. In this example, we set $n=10$ and $m=2$. We repeat our sampling trials by 50 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the experiment\n",
    "run(Niter = 50,\n",
    "    directory = 'results/posterior_predictive_distr_by_sampling',\n",
    "    n = 10,\n",
    "    m = 2,\n",
    "    r = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Posterior predictive distribution by sampling](results/posterior_predictive_distr_by_sampling_i50_n10_m2_r10/animation.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the left-hand plot, we have the posterior distribution, which is a $\\beta(3,\\,9)$ distribution. In the middle plot, we show a binomial probability distribution which is conditional on the $\\theta$ value we sample from the $\\beta$ posterior distribution. The value sampled from the $\\beta$ posterior in each trial is indicated by a vertical orange line. After sampling the $\\theta$ value from the posterior, we can plot what a binomial distribution with a sample size of $n=10$ and a probability of $\\theta$ looks like. What we do next is to sample a value of $\\tilde{X}$ from the corresponding binomial distribution. This is indicated again by a vertical orange line passing through the bin from which the value of $\\tilde{X}$ is extracted and which goes into building up the histogram of our posterior predictive distribution on the right. \n",
    "\n",
    "For every iteration, we build up a new binomial distribution depending on the value of $\\theta$ which is randomly obtained from the posterior $\\beta$ distribution. Then in turn we sample a new value $\\tilde{X}$ conditional on the sampling distribution assuming that $\\theta$. As we do that, we build up an approximated posterior predictive distribution which starts to converge to a given distributional shape, as shown on the right.\n",
    "\n",
    "Because the posterior $\\beta$ distribution is centered on about 0.2, our posterior predictive distribution is unsurprisingly centered on about $m/n=2/10=0.2$. So, we would predict about 2 individuals carrying glasses out of 10 if we are to collect a sample of data.\n",
    "\n",
    "Let's now illustrate this process for a different posterior  distribution. We can assume we have collected a sample size of $n=1000$ students and out of those we found $m=200$ of them wearing myopia glasses. If we use the same uniform prior as we used before, then we obtain a $\\beta$ posterior distribution with $a=1+m=201$ and $b=1+n-m=801$, i.e. $\\beta(201,\\, 801)$. Because we have collected a much bigger sample size, our posterior is going to be much more narrow in its core region around $\\theta=0.2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the experiment with a different posterior\n",
    "run(Niter = 50,\n",
    "    directory = 'results/posterior_predictive_distr_by_sampling',\n",
    "    n = 1000,\n",
    "    m = 200,\n",
    "    r = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Posterior predictive distribution by sampling](results/posterior_predictive_distr_by_sampling_i50_n1000_m200_r10/animation.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run this process, we observe that the values of $\\theta$ we are sampling tend to be very close to one another and centered around 0.2. Consequently, our binomial sampling distribution isn not changing drastically between iterations. Even though we have collected an bigger original sample of 1000 students, we can still predict the number of students out of 10 in our new, predicted sample in order to be able to read off and compare. On the right, we are again showing our predictive posterior distribution which is assuming that in the new sample has size $n=10$.\n",
    "\n",
    "We observe that after some time, the posterior predictive distribution takes a certain distributional shape and it converges to a distribution which looks similar to our sampling distribution.\n",
    "\n",
    "To compare, we can show the two examples one on top of the other as presented below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Posterior predictive distribution by sampling](results/posterior_predictive_distr_by_sampling_i50_n10_m2_r10/animation.gif)\n",
    "![Posterior predictive distribution by sampling](results/posterior_predictive_distr_by_sampling_i50_n1000_m200_r10/animation.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top row of plots corresponds to the first example where we obtained a sample size of $(n=10,\\,m=2)$ whereas the bottom plots represent the outcome for a sample size of $(n=1000,\\,m=200)$. \n",
    "\n",
    "We can see that the posterior between these two examples is quite different with the second one being obviously much more peaky. However, on the right histograms we do not notice such big differences in the posterior predictive distribution. Of course, the bottom right data histogram is slightly more narrow and peaked at 2, but there aren't that marked differences between the two posterior predictive distributions. What could be the underlying reason for that? The posterior predictive distribution represent the uncertainty over two different causes:\n",
    "\n",
    "1. the uncertainty over the value of the unknown parameter $\\theta$. This is represented by the posterior in the first step when we sample a value $\\theta$ from the posterior.\n",
    "2. the uncertainty coming from the binomial distribution itself. The binomial distribution we sample from is relatively insensitive to the value of $\\theta$ that we are sampling across these two examples.\n",
    "\n",
    "The posterior predictive distribution is overall representing mostly the uncertainty in the sampling distribution shown in the middle by the binomial, because that tends to be the dominant source of uncertainty. \n",
    "\n",
    "## Summary\n",
    "\n",
    "The posterior predictive distribution is a distribution that tends to be too difficult to calculate analytically. Therefore, in most applied circumstances we obtain an approximate posterior predictive distribution using sampling. In the first step, we sample a value of $\\theta$ from a posterior distribution and then conditional on that value of theta we sample a value of data in a new sample, $\\tilde{X}$. If this process is repeated a large number of times, we then obtain an approximate posterior predictive distribution. The posterior predictive distribution is representative of the uncertainty of two sources: (a) the source of uncertainty over the value of the parameter $\\theta$ and (b) the source of uncertainty coming from the sampling distribution itself, which represents the uncertainty over the process which generates data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- Posterior predictive distribution, [Wikipedia article](https://en.wikipedia.org/wiki/Posterior_predictive_distribution)\n",
    "\n",
    "- A Student's Guide to Bayesian Statistics, Ben Lambert, [Book on Amazon](https://www.amazon.de/-/en/Ben-Lambert/dp/1473916364/ref=sr_1_1?dchild=1&keywords=A+Student%27s+Guide+to+Bayesian+Statistics&qid=1609326094&sr=8-1), Chapter 7.8. Youtube [channel](https://www.youtube.com/playlist?list=PLwJRxp3blEvZ8AKMXOy0fc0cqT61GsKCG)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
